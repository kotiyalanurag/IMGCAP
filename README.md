<h1 align=center>ðŸ˜¬: IMGCAP

![](https://img.shields.io/badge/Python-3.9-blue) ![](https://img.shields.io/badge/torch-2.1.2-blue) ![](https://img.shields.io/badge/Contributions-Welcome-brightgreen) ![](https://img.shields.io/badge/LICENSE-MIT-red)</h1>

<p align = left>Using a CNN plus RNN-based architecture to generate captions for images - trained on <a href='https://www.kaggle.com/datasets/adityajn105/flickr8k'>Flickr8k</a> dataset.</p>

## Overview 

Image captioning i.e., automatically generating natural language descriptions according to the content observed in an image, is an important part of scene understanding, which combines knowledge from computer vision and natural language processing. This is a very basic implementation of a model architecture capable of generating captions for given images.

## Results

Given below are some captions that were generated by the model on test images. 

<p align="center">
  <img src = test/boat.png max-width=100% height='295' />
</p>

<p align='center'>
<strong>Correct tag:</strong> A small  boat in the ocean <br>
<strong>Prediction:</strong> a man is rowing a canoe through lake
</p>

<p align="center">
  <img src = test/horse.png max-width=100% height='300' />
</p>

<p align='center'>
<strong>Correct tag:</strong> A cowboy riding a horse in the desert <br>
<strong>Prediction:</strong> a group of dogs in a field
</p>


<p align="center">
  <img src = test/dog.jpg max-width=100% height='330' />
</p>
<p align='center'>
<strong>Correct tag:</strong> Dog on a beach by the ocean <br>
<strong>Prediction:</strong> a brown dog running through water
</p>

As we can see from the generated captions, there is a lot of scope for improvement in this model since it was only trained for 15 epochs due to time constraints. On an m1 silicon chip mac it almost took 3h to train this model.

## Hyperparameters

Can be changed inside **'train.py'** file
```python
embed_size = 256
hidden_state = 256
num_layers = 1
lr = 3e-4
epochs = 5
```
If you have cuda then change line 24 inside **'train.py'** file
```python
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
```
## How to get started?

```html
mkdir IMGCAP
```
Then switch to the new directory using
```html
cd IMGCAP
```
Now clone the repository on your local machine using
```html
git clone https://github.com/kotiyalanurag/IMGCAP.git
```
Now create a virtual environment using (assuming you have python installed on your machine)
```html
python -m venv env
```
Switch to your new virtual environment using
```html
source env/bin/activate
```
Now install the requirements for this project using
```html
pip install -r requirements.txt
```
To start the model training run the following command
```html
python3 train.py
```